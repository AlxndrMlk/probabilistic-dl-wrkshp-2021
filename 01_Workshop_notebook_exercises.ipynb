{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    \n",
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFP docs are [here](https://www.tensorflow.org/probability/api_docs/python/tfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Bayesian neural networks with TF and TFP\n",
    "\n",
    "From medical research to small-data scenarios, whenever we want to understand how sure the model is about its own predictions, modeling uncertainty can be immensely helpful. During the workshop weâ€™ll learn how to build Bayesian neural networks using Tensorflow and Tensorflow Probability to model uncertainty. At the end of the workshop, youâ€™ll have practical knowledge how to create basic types of Bayesian neural network using Tensorflow ecosystem and you'll be able to apply these techniques to your own projects. \n",
    "\n",
    "To fully benefit from the workshop you need: \n",
    "- good practical knowledge of Python\n",
    "- practical understanding of deep learning principles \n",
    "- experience using Tensorflow (recommended) or other contemporary deep learning framework \n",
    "- good understanding of basic probability and basic distributions \n",
    "- familiarity with Bayes' theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TFP Components: `tfp.distributions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with Normal(0, 1)\n",
    "normal = tfd.Normal(loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Methods -  `.sample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from `normal`\n",
    "normal.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample form `normal` and cast the result to a numpy array\n",
    "normal.sample(10).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Exercise 1.1 - sample and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a normal distribution with mean 100 and sd 15\n",
    "normal = None\n",
    "\n",
    "# Draw 2000 samples form `normal`\n",
    "samples = None\n",
    "\n",
    "# Plot a histogram of `samples`\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(samples.numpy(), density=True, bins=np.sqrt(2000).astype(int))\n",
    "plt.title('A histogram of 500 samples\\nfrom $N(100, 15)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Methods -  `.prob()` & `.log_prob()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare PDF values of a couple of values given `normal`\n",
    "values = [70., 85., 100., 115., 130.]\n",
    "\n",
    "for v in values:\n",
    "    print(f'PDF of {v:05.1f} == {normal.prob(v):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's do the same for log_probs\n",
    "values = [70., 85., 100., 115., 130.]\n",
    "\n",
    "for v in values:\n",
    "    print(f'Log PDF {v:05.1f} == {normal.log_prob(v):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Exercise 1.2 - evaluate prob and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Beta distribution (`tfd.Beta`) with `concetration1=2.` and `concentration0=5.`\n",
    "beta = None # YOUR CODE HERE\n",
    "\n",
    "# Sample 2000 samples from `beta` and plot a histogram of these samples\n",
    "samples = None # YOUR CODE HERE\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(samples.numpy(), bins=np.sqrt(2000).astype(int), density=True)\n",
    "plt.title('A histogram of 2000 samples\\nfrom $B(2, 5)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PDF and logPDF of 2000 samples drawn from `beta` usin `.prob()` and `.log_prob()` methods\n",
    "beta_prob = None # YOUR CODE HERE\n",
    "beta_log_prob = None # YOUR CODE HERE\n",
    "\n",
    "# Plot samples vs their log_probs using a scatter plot\n",
    "# On the same scatter plot, plot samples vs their respective probs\n",
    "# Add labels, legend and label your axes\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(samples.numpy(), beta_prob.numpy(), alpha=.2, label='prob')\n",
    "plt.scatter(samples.numpy(), beta_log_prob.numpy(), alpha=.2, label='log prob')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$pdf(x)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Shapes - batch & events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a multivariate normal\n",
    "normal_batch = tfd.Normal(loc=[1., 0.], scale=1.)\n",
    "normal_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's let's turn it into a 2d normal using `tfd.Independent`\n",
    "mvn = tfd.Independent(normal_batch, reinterpreted_batch_ndims=1)\n",
    "mvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Exercise 1.3 - build a multivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an MVN with batch size of 2 and event shape of 3 using `tfd.Normal` and `tfd.Independent`\n",
    "# Define an array of means (loc) of shape 2x3\n",
    "means = None # YOUR CODE HERE\n",
    "\n",
    "# Build a normal distr with batch size 2x3 use `means` as your loc, set `scale` to 1.\n",
    "normal_batched = None # YOUR CODE HERE\n",
    "normal_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap your normal 2x3 using `tfd.Independent` \n",
    "# and reinterpret some batch dims to get the event shape of 3 while keeping batch shape of 2\n",
    "mvn = None # YOUR CODE HERE\n",
    "mvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modleing aleatoric uncertainty - `tfp.layers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `tfd.DistributionLambda` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate some data! \n",
    "\n",
    "Our data generating process will be defined by the following formula:\n",
    "\n",
    "$$\\large y = 1.5x + 3 + 0.35\\epsilon$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\epsilon \\sim N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "x_train = np.linspace(-1, 1, 200)[:, np.newaxis]\n",
    "y_train = 1.5*x_train + 3 + 0.35*np.random.randn(200)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, alpha=.5)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a simple linear regression model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "])\n",
    "\n",
    "# Compile \n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# Fit\n",
    "history = model.fit(x_train, y_train, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and a trained regression line\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, alpha=.5)\n",
    "plt.plot(x_train, y_pred, label='Fitted reg. line', c='r')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Training data & fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('No. of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Model with probabilistic output - aleatoric uncertainty (learn mean & std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a model with probabilistic output (mean & std)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, input_shape=(1,)),\n",
    "    tfpl.DistributionLambda(\n",
    "        lambda t: tfd.Independent(\n",
    "            tfd.Normal(\n",
    "                loc=t[..., :1],\n",
    "                scale=tf.math.softplus(t[..., 1:])\n",
    "            ),\n",
    "            reinterpreted_batch_ndims=1\n",
    "        )\n",
    "    )\n",
    "])\n",
    "\n",
    "# Define neg. loglik. loss function\n",
    "def neg_loglik(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "# Compile \n",
    "model.compile(loss=neg_loglik, optimizer='sgd')\n",
    "\n",
    "# Fit\n",
    "history = model.fit(x_train, y_train, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare distributions - y_train & y_model\n",
    "y_model = model(x_train).sample().numpy()\n",
    "\n",
    "# Plot \n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, alpha=.5, label='Training data')\n",
    "plt.scatter(x_train, y_model, alpha=.5, label='Learned dist. samples')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend()\n",
    "plt.title('Training data vs. learned dist.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conf ints\n",
    "y_sample = model(x_train).sample()\n",
    "y_hat = model(x_train).mean()\n",
    "y_sd = model(x_train).stddev()\n",
    "y_hat_lower = y_hat - 2 * y_sd\n",
    "y_hat_upper = y_hat + 2 * y_sd\n",
    "\n",
    "# Plot conf ints\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, alpha=.5)\n",
    "plt.plot(x_train, y_hat, label='Fitted reg. line', c='r')\n",
    "plt.fill_between(np.squeeze(x_train), np.squeeze(y_hat_lower), np.squeeze(y_hat_upper), alpha=.1, label='$+/- 2SD$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Training data & fitted line\\n+ 95% CIs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 A simpler way to model aleatoric uncertainty - `tfd.layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a model with probabilistic output (mean & std) - a simpler way\n",
    "event_shape = 1\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=tfpl.IndependentNormal.params_size(event_shape), input_shape=(1,)),\n",
    "    tfpl.IndependentNormal(event_shape=event_shape)\n",
    "])\n",
    "\n",
    "# Define neg. loglik. loss function\n",
    "def neg_loglik(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "# Compile \n",
    "model.compile(loss=neg_loglik, optimizer='sgd')\n",
    "\n",
    "# Fit\n",
    "history = model.fit(x_train, y_train, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conf ints\n",
    "y_sample = model(x_train).sample()\n",
    "y_hat = model(x_train).mean()\n",
    "y_sd = model(x_train).stddev()\n",
    "y_hat_lower = y_hat - 2 * y_sd\n",
    "y_hat_upper = y_hat + 2 * y_sd\n",
    "\n",
    "# Plot conf ints\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, alpha=.5)\n",
    "plt.plot(x_train, y_hat, label='Fitted reg. line', c='r')\n",
    "plt.fill_between(np.squeeze(x_train), np.squeeze(y_hat_lower), np.squeeze(y_hat_upper), alpha=.1, label='$+/- 2SD$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Training data & fitted line\\n+ 95% CIs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Exercise 2.1 - build a model to estimate aleatoric uncertainty in a non-linear case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linear data\n",
    "n_samples = int(5e3)\n",
    "x_train = np.linspace(-1, 1, n_samples)[:, np.newaxis]\n",
    "y_train = x_train**3 + 0.1*(1.5 + x_train)*np.random.randn(n_samples)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, alpha=0.05)\n",
    "plt.title('Non-linear training data')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a model with probabilistic output (mean & std) - a simpler way\n",
    "event_shape = 1\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Add a regular tf layer with non-linearity\n",
    "    None # YOUR CODE HERE\n",
    "    # Add a layer to parametrize the output distribution\n",
    "    None # YOUR CODE HERE\n",
    "    # Add the output layer (use `tfd.IndependentNormal` with `event_shape=event_shape`)\n",
    "    None # YOUR CODE HERE\n",
    "])\n",
    "\n",
    "# Define neg. loglik. loss function\n",
    "def neg_loglik(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "# Compile \n",
    "model.compile(loss=neg_loglik, optimizer='sgd')\n",
    "\n",
    "# Fit\n",
    "history = model.fit(x_train, y_train, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('No. of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conf ints\n",
    "y_sample = model(x_train).sample()\n",
    "y_hat = model(x_train).mean()\n",
    "y_sd = model(x_train).stddev()\n",
    "y_hat_lower = y_hat - 2 * y_sd\n",
    "y_hat_upper = y_hat + 2 * y_sd\n",
    "\n",
    "# Plot conf ints\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, alpha=.1)\n",
    "plt.plot(x_train, y_hat, label='Fitted reg. line', c='r')\n",
    "plt.fill_between(np.squeeze(x_train), np.squeeze(y_hat_lower), np.squeeze(y_hat_upper), alpha=.4, label='$+/- 2SD$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Training data & fitted curve\\n+ 95% CIs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling epistemic uncertainty - `tfpl.DenseVariational`\n",
    "\n",
    "Now, we're going to model weight's uncertainty. \n",
    "\n",
    "This means, that each weight in our network will now be represented by a **distribution**, and **not** just a **point estimate**.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://sanjaykthakur.files.wordpress.com/2018/12/bayes_nn.png\">\n",
    "<p style=\"text-align: center\"><sup>Image from <a href=\"https://sanjaykthakur.com/2018/12/05/the-very-basics-of-bayesian-neural-networks/\">https://sanjaykthakur.com/2018/12/05/the-very-basics-of-bayesian-neural-networks/</a></sup></p>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "To do this we'll use **Bayes by Backprop** method introduced in a Blundell et al. paper [Weight uncertainty in neural networks](https://arxiv.org/pdf/1505.05424.pdf) (2015).\n",
    "\n",
    "<br>\n",
    "\n",
    "We'll try to estimate weight distribution parameters $\\theta$, given data $D$:\n",
    "\n",
    "\n",
    "$$\\large P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)} $$\n",
    "\n",
    "\n",
    "We'll follow a three-step formula to achieve this:\n",
    "\n",
    "1. Pick a prior density over weights $P(\\theta)$\n",
    "2. Use training data $D$ to determine the likelihood $P(D | \\theta)$\n",
    "3. Estimate the posterior density over weights $P(\\theta | D)$ \n",
    "\n",
    "<br>\n",
    "\n",
    "Keywords: [***KL divergence***](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), [***ELBO***](https://en.wikipedia.org/wiki/Evidence_lower_bound), [***reparametrization trick***](https://gregorygundersen.com/blog/2018/04/29/reparameterization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Epistemic uncertainty - linear case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with our linear data!\n",
    "x_train_100 = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
    "x_train_1000 = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
    "\n",
    "y_train_100 = 1.5*x_train_100 + 3 + 0.35*np.random.randn(100)[:, np.newaxis]\n",
    "y_train_1000 = 1.5*x_train_1000 + 3 + 0.35*np.random.randn(1000)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(x_train_100, y_train_100, alpha=.5)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$n=100$', fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(x_train_1000, y_train_1000, alpha=.5)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$n=1000$', fontsize=14)\n",
    "\n",
    "plt.suptitle('Training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Define prior and posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prior()` and `posterior()` functions are necessary to parametrize `tfpl.DenseVariational` layer.\n",
    "\n",
    "They both need to take `kernel_size`, `bias_size` and `dtype` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior - diagonal MVN ~ N(0, 1)\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    \n",
    "    n = kernel_size + bias_size\n",
    "    \n",
    "    prior_model = tf.keras.Sequential([\n",
    "        \n",
    "        tfpl.DistributionLambda(\n",
    "            # Non-trianable distribution\n",
    "            lambda t: tfd.MultivariateNormalDiag(loc=tf.zeros(n), scale_diag=tf.ones(n)))\n",
    "    ])\n",
    "    \n",
    "    return prior_model\n",
    "\n",
    "\n",
    "# Posterior\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    \n",
    "    n = kernel_size + bias_size\n",
    "    \n",
    "    posterior_model = tf.keras.Sequential([\n",
    "        \n",
    "        tfpl.VariableLayer(tfpl.MultivariateNormalTriL.params_size(n), dtype=dtype),\n",
    "        tfpl.MultivariateNormalTriL(n)\n",
    "    ])\n",
    "    \n",
    "    return posterior_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Aleatoric uncertainty - define a model with `DenseVariational` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def get_aleatoric_model(x_train_shape):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tfpl.DenseVariational(\n",
    "            1,\n",
    "            input_shape=(1,),\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=1/x_train_shape, # Normalizing to scale the D_KL term in ELBO properly when using minibatches.\n",
    "            kl_use_exact=False) # could be `True` in this case, but we go for estimated value\n",
    "    ])\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Summarize\n",
    "model = get_aleatoric_model(100)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Exercise 3.1 - understanding the number of parameters\n",
    "\n",
    "In the summary above we see that we got 5 trainable parameters.\n",
    "\n",
    "Our model is just a simple linear regression. What are those 5 parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on two datasets \n",
    "print('Fitting `model_100`...')\n",
    "model_100 = get_aleatoric_model(100)\n",
    "history_100 = model_100.fit(x_train_100, y_train_100, epochs=500, verbose=False)\n",
    "\n",
    "print('Fitting `model_1000`...')\n",
    "model_1000 = get_aleatoric_model(1000)\n",
    "history_1000 = model_1000.fit(x_train_1000, y_train_1000, epochs=500, verbose=False)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history_100.history['loss'], label='model_100', alpha=.5)\n",
    "plt.plot(history_1000.history['loss'], label='model_1000', alpha=.5)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('No. of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fitted regression lines\n",
    "N_ITERS = 15\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(x_train_100, y_train_100, alpha=.5, label='Training data')\n",
    "for _ in range(N_ITERS):\n",
    "    y_model_100 = model_100(x_train_100)\n",
    "    if _ == 0:\n",
    "        plt.plot(x_train_100, y_model_100, color='red', alpha=0.5, label='Fitted reg. lines')\n",
    "    else:\n",
    "        plt.plot(x_train_100, y_model_100, color='red', alpha=0.5)        \n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$n=100$', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(x_train_1000, y_train_1000, alpha=.5, label='Training data')\n",
    "for _ in range(N_ITERS):\n",
    "    y_model_1000 = model_1000(x_train_1000)\n",
    "    if _ == 0:\n",
    "        plt.plot(x_train_1000, y_model_1000, color='red', alpha=0.5, label='Fitted reg. lines')\n",
    "    else:\n",
    "        plt.plot(x_train_1000, y_model_1000, color='red', alpha=0.5)        \n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$n=1000$', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "plt.suptitle('Training data and fitted lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Going fully probabilistic: aleatoric & epistemic uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fully probabilistic non-linear case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linear data\n",
    "n_samples = int(5e3)\n",
    "x_train = np.linspace(-1, 1, n_samples)[:, np.newaxis]\n",
    "y_train = x_train**3 + 0.1*(1.5 + x_train)*np.random.randn(n_samples)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, alpha=0.05)\n",
    "plt.title('Non-linear training data')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior - diagonal MVN ~ N(0, 1)\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    \n",
    "    n = kernel_size + bias_size\n",
    "    \n",
    "    prior_model = tf.keras.Sequential([\n",
    "        \n",
    "        tfpl.DistributionLambda(\n",
    "            # Non-trianable distribution\n",
    "            lambda t: tfd.MultivariateNormalDiag(loc=tf.zeros(n), scale_diag=tf.ones(n)))\n",
    "    ])\n",
    "    \n",
    "    return prior_model\n",
    "\n",
    "\n",
    "# Posterior\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    \n",
    "    n = kernel_size + bias_size\n",
    "    \n",
    "    posterior_model = tf.keras.Sequential([\n",
    "        tfpl.VariableLayer(tfpl.MultivariateNormalTriL.params_size(n), dtype=dtype),\n",
    "        tfpl.MultivariateNormalTriL(n)\n",
    "    ])\n",
    "    \n",
    "    return posterior_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def get_full_model(x_train_shape):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "\n",
    "        # Epistemic uncertainty\n",
    "        tfpl.DenseVariational(units=8,\n",
    "                              input_shape=(1,),\n",
    "                              make_prior_fn=prior,\n",
    "                              make_posterior_fn=posterior,\n",
    "                              kl_weight=1/x_train_shape,\n",
    "                              kl_use_exact=False,\n",
    "                              activation='sigmoid'),\n",
    "        \n",
    "        tfpl.DenseVariational(units=tfpl.IndependentNormal.params_size(1),\n",
    "                              make_prior_fn=prior,\n",
    "                              make_posterior_fn=posterior,\n",
    "                              kl_use_exact=False,\n",
    "                              kl_weight=1/x_train_shape),\n",
    "\n",
    "        # Aleatoric uncertainty\n",
    "        tfpl.IndependentNormal(1)\n",
    "    ])\n",
    "\n",
    "    def neg_loglik(y_true, y_pred):\n",
    "        return -y_pred.log_prob(y_true)\n",
    "\n",
    "    model.compile(loss=neg_loglik, optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "model_full = get_full_model(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize the model\n",
    "model_full = get_full_model(n_samples)\n",
    "\n",
    "# Fit\n",
    "history = model_full.fit(x_train, y_train, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('No. of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fitted regression lines\n",
    "N_ITERS = 15\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=.1, label='Training data')\n",
    "for _ in range(N_ITERS):\n",
    "    \n",
    "    # Compute conf ints\n",
    "    y_sample = model_full(x_train).sample()\n",
    "    y_hat = model_full(x_train).mean()\n",
    "    y_sd = model_full(x_train).stddev()\n",
    "    y_hat_lower = y_hat - 2 * y_sd\n",
    "    y_hat_upper = y_hat + 2 * y_sd\n",
    "    \n",
    "\n",
    "    if _ == 0:\n",
    "        plt.plot(x_train, y_hat, color='red', alpha=.5, lw=1, label='Fitted reg. lines')\n",
    "        plt.plot(x_train, y_hat_lower, c='g', alpha=.5, lw=1, label='95% CI')\n",
    "        plt.plot(x_train, y_hat_upper, c='g', alpha=.5, lw=1)\n",
    "    else:\n",
    "        plt.plot(x_train, y_hat, color='red', alpha=.5, lw=1)    \n",
    "        plt.plot(x_train, y_hat_lower, c='g', alpha=.5, lw=1)\n",
    "        plt.plot(x_train, y_hat_upper, c='g', alpha=.5, lw=1)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend()\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "plt.title(f'Training data + fitted lines + 95% CIs\\nn={n_samples}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Survey\n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "ðŸ’– Thank you for **your participation**!\n",
    "\n",
    "Please let me know what are **your thoughts** on the workshop [**here**](https://forms.gle/uyiC3SogtFzYNim26)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-proba-env]",
   "language": "python",
   "name": "conda-env-tf-proba-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
